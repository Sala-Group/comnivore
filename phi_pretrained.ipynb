{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Phi import Extractor_CLIP, Phi\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "from Generate_data_augmentation import *\n",
    "\n",
    "from wilds.datasets.camelyon17_dataset import Camelyon17Dataset\n",
    "from wilds.datasets.domainnet_dataset import DomainNetDataset\n",
    "from wilds.datasets.waterbirds_dataset import WaterbirdsDataset\n",
    "from wilds.datasets.wilds_dataset import WILDSSubset\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hidden = 20\n",
    "dataset_name = \"waterbirds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip to instantiate a model of type clip_vision_model. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'logit_scale', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'visual_projection.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "source": [
    "extractor_model = Extractor_CLIP(z_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1280\n",
    "root_dir = \"/hdd2/wilds_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDomainnet(DomainNetDataset):\n",
    "    def __init__(self, **dataset_kwargs):\n",
    "        super().__init__(split_scheme ='official', use_sentry=True, **dataset_kwargs)\n",
    "    \n",
    "    def get_input(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self._input_image_paths[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self._y_array.numpy()[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def get_subset(self, split, frac=1.0, transform=None):\n",
    "        if split not in self.split_dict:\n",
    "            raise ValueError(f\"Split {split} not found in dataset's split_dict.\")\n",
    "\n",
    "        split_mask = self.split_array == self.split_dict[split]\n",
    "        split_idx = np.where(split_mask)[0]\n",
    "\n",
    "        if frac < 1.0:\n",
    "            # Randomly sample a fraction of the split\n",
    "            num_to_retain = int(np.round(float(len(split_idx)) * frac))\n",
    "            split_idx = np.sort(np.random.permutation(split_idx)[:num_to_retain])\n",
    "\n",
    "        return WILDSSubset(self, split_idx, transform, do_transform_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWaterbirds(WaterbirdsDataset):\n",
    "    def __init__(self, **dataset_kwargs):\n",
    "        super().__init__(split_scheme ='official', **dataset_kwargs)\n",
    "    \n",
    "    def get_input(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self._input_array[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self._y_array.numpy()[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def get_subset(self, split, frac=1.0, transform=None):\n",
    "        if split not in self.split_dict:\n",
    "            raise ValueError(f\"Split {split} not found in dataset's split_dict.\")\n",
    "\n",
    "        split_mask = self.split_array == self.split_dict[split]\n",
    "        split_idx = np.where(split_mask)[0]\n",
    "\n",
    "        if frac < 1.0:\n",
    "            # Randomly sample a fraction of the split\n",
    "            num_to_retain = int(np.round(float(len(split_idx)) * frac))\n",
    "            split_idx = np.sort(np.random.permutation(split_idx)[:num_to_retain])\n",
    "\n",
    "        return WILDSSubset(self, split_idx, transform, do_transform_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCamelyon(Camelyon17Dataset):\n",
    "    def __init__(self, **dataset_kwargs):\n",
    "        super().__init__(split_scheme ='official', **dataset_kwargs)\n",
    "    \n",
    "    def get_input(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self._input_array[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self._y_array.numpy()[idx]\n",
    "        return img, label\n",
    "    \n",
    "    def get_subset(self, split, frac=1.0, transform=None):\n",
    "        if split not in self.split_dict:\n",
    "            raise ValueError(f\"Split {split} not found in dataset's split_dict.\")\n",
    "\n",
    "        split_mask = self.split_array == self.split_dict[split]\n",
    "        split_idx = np.where(split_mask)[0]\n",
    "\n",
    "        if frac < 1.0:\n",
    "            # Randomly sample a fraction of the split\n",
    "            num_to_retain = int(np.round(float(len(split_idx)) * frac))\n",
    "            split_idx = np.sort(np.random.permutation(split_idx)[:num_to_retain])\n",
    "\n",
    "        return WILDSSubset(self, split_idx, transform, do_transform_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'waterbirds':\n",
    "    dataset=CustomWaterbirds(download=False, root_dir=root_dir)\n",
    "elif dataset_name == 'domainnet':\n",
    "    dataset=CustomDomainnet(download=False, root_dir=root_dir)\n",
    "else:\n",
    "    dataset=CustomCamelyon(download=False, root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'domainnet':\n",
    "    dataset_orig = get_dataset(dataset=dataset_name, download=False, root_dir=root_dir, use_sentry=True)\n",
    "else:\n",
    "    dataset_orig = get_dataset(dataset=dataset_name, download=False, root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0, 'val': 1, 'test': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orig.split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>img_filename</th>\n",
       "      <th>y</th>\n",
       "      <th>split</th>\n",
       "      <th>place</th>\n",
       "      <th>place_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6937</th>\n",
       "      <td>6938</td>\n",
       "      <td>119.Field_Sparrow/Field_Sparrow_0111_113899.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/f/forest/broadleaf/00001033.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9028</th>\n",
       "      <td>9029</td>\n",
       "      <td>154.Red_eyed_Vireo/Red_Eyed_Vireo_0101_156988.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/o/ocean/00003200.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>4142</td>\n",
       "      <td>072.Pomarine_Jaeger/Pomarine_Jaeger_0024_61281...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/o/ocean/00002901.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>3739</td>\n",
       "      <td>065.Slaty_backed_Gull/Slaty_Backed_Gull_0020_7...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/o/ocean/00001006.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>1684</td>\n",
       "      <td>030.Fish_Crow/Fish_Crow_0079_26030.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/f/forest/broadleaf/00001217.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      img_id                                       img_filename  y  split  \\\n",
       "6937    6938    119.Field_Sparrow/Field_Sparrow_0111_113899.jpg  0      0   \n",
       "9028    9029  154.Red_eyed_Vireo/Red_Eyed_Vireo_0101_156988.jpg  0      0   \n",
       "4141    4142  072.Pomarine_Jaeger/Pomarine_Jaeger_0024_61281...  1      0   \n",
       "3738    3739  065.Slaty_backed_Gull/Slaty_Backed_Gull_0020_7...  1      0   \n",
       "1683    1684             030.Fish_Crow/Fish_Crow_0079_26030.jpg  0      0   \n",
       "\n",
       "      place                    place_filename  \n",
       "6937      0  /f/forest/broadleaf/00001033.jpg  \n",
       "9028      1             /o/ocean/00003200.jpg  \n",
       "4141      1             /o/ocean/00002901.jpg  \n",
       "3738      1             /o/ocean/00001006.jpg  \n",
       "1683      0  /f/forest/broadleaf/00001217.jpg  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_map_file = os.path.join(root_dir, f\"{dataset_name}_v1.0\", \"metadata.csv\")\n",
    "categories = pd.read_csv(categories_map_file)\n",
    "categories.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(categories['split'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.unique(categories['y'].tolist())\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCompose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, tar):\n",
    "        for i, t in enumerate(self.transforms):\n",
    "            try:\n",
    "                if t._get_name() == \"Custom\":\n",
    "                    img = t(img, tar)\n",
    "                else:\n",
    "                    img = t(img[0])\n",
    "            except Exception as e:\n",
    "                img = t(img)\n",
    "        return img, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataloader(transform):\n",
    "    dataset_ = dataset.get_subset(split=\"train\", transform=transform)\n",
    "    return DataLoader(dataset_, batch_size=batch_size,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_mapping = generate_transformation_mapping(labels, cmaps)\n",
    "trainloader_color = get_train_dataloader(\n",
    "    MyCompose(\n",
    "        [transforms.Resize((448, 448)), Change_cmap(cmap_mapping) , transforms.ToTensor()]\n",
    "    ))\n",
    "\n",
    "rotation_mapping = generate_transformation_mapping(labels, angles)\n",
    "trainloader_rotation = get_train_dataloader(\n",
    "    MyCompose(\n",
    "        [transforms.Resize((448, 448)), Rotate_Image(rotation_mapping), transforms.ToTensor()]\n",
    "    ))\n",
    "\n",
    "\n",
    "zoom_mapping = generate_transformation_mapping(labels, zoom_factors)\n",
    "trainloader_zoom = get_train_dataloader(\n",
    "    MyCompose(\n",
    "        [transforms.Resize((448, 448)), Zoom_Image(zoom_mapping), transforms.ToTensor()]\n",
    "    ),)\n",
    "\n",
    "shift_mapping = generate_transformation_mapping(labels, shift_factors)\n",
    "trainloader_shift = get_train_dataloader(MyCompose(\n",
    "        [transforms.Resize((448, 448)), Shift_Image(shift_mapping), transforms.ToTensor()]\n",
    "    ),)\n",
    "\n",
    "\n",
    "trainloader_mix_1 = get_train_dataloader(MyCompose(\n",
    "        [transforms.Resize((448, 448)), \n",
    "         Rotate_Image(rotation_mapping), \n",
    "         Shift_Image(shift_mapping), transforms.ToTensor()]\n",
    "    ),)\n",
    "\n",
    "\n",
    "traindata_orig = dataset_orig.get_subset(split=\"train\", transform=transforms.Compose(\n",
    "        [transforms.Resize((448, 448)), transforms.ToTensor()]))\n",
    "trainloader_orig = DataLoader(traindata_orig, batch_size=batch_size,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_mix_2 = get_train_dataloader(MyCompose(\n",
    "        [transforms.Resize((448, 448)), Zoom_Image(zoom_mapping), Shift_Image(shift_mapping), transforms.ToTensor()]\n",
    "    ),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_mapping = generate_transformation_mapping(labels, cmaps)\n",
    "trainloader_color_2 = get_train_dataloader(MyCompose(\n",
    "        [transforms.Resize((448, 448)), Change_cmap(cmap_mapping) , transforms.ToTensor()]\n",
    "    ),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_gaussian = get_train_dataloader(MyCompose(\n",
    "        [transforms.Resize((448, 448)), Apply_Filter('gaussian'), transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_spline = get_train_dataloader(MyCompose(\n",
    "        [transforms.Resize((448, 448)), Apply_Filter('spline'), transforms.ToTensor()]))\n",
    "trainloader_uniform = get_train_dataloader(MyCompose(\n",
    "        [transforms.Resize((448, 448)), Apply_Filter('uniform'), transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(loader, cols=6):\n",
    "    fig, axs = plt.subplots(1, cols, figsize=(10,12), constrained_layout=True)\n",
    "    for batch_idx, x in enumerate(loader):\n",
    "        if batch_idx > 0:\n",
    "            break\n",
    "        imgs, labels = x[0], x[1]\n",
    "        n_images = imgs.shape[0]\n",
    "        cols = 6\n",
    "        axs = axs.flatten()\n",
    "        random_idx = np.random.choice(np.arange(imgs.shape[0]), 6, replace=False)\n",
    "        img_sample = imgs[random_idx]\n",
    "        for i, ax in enumerate(axs):\n",
    "            img = img_sample[i]\n",
    "#             print(img.shape)\n",
    "            axs[i].imshow(img.permute(1,2,0))\n",
    "            y = labels[random_idx][i].item()\n",
    "            axs[i].set_title(y, fontsize=10)\n",
    "            axs[i].set_xticks([])\n",
    "            axs[i].set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_samples(trainloader_uniform, cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_samples(trainloader_gaussian, cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_samples(trainloader_color, cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_samples(trainloader_rotation, cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_samples(trainloader_zoom, cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_samples(trainloader_shift, cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_samples(trainloader_orig, cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_samples(trainloader_mix_1, cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (c, r, z, s, o) in tqdm(enumerate(zip(trainloader_color, trainloader_rotation, trainloader_zoom, trainloader_shift, trainloader_orig))):\n",
    "#     if i > 1:\n",
    "#         break\n",
    "#     colored = c[0].cpu().detach().numpy()\n",
    "#     rotated = r[0].cpu().detach().numpy()\n",
    "#     zoomed = z[0].cpu().detach().numpy()\n",
    "#     shifted = s[0].cpu().detach().numpy()\n",
    "#     original = o[0].cpu().detach().numpy()\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     for j, item in enumerate(colored):\n",
    "#         if j >= 9: break\n",
    "#         plt.subplot(5, 9, j+1)\n",
    "#         im2display = item.transpose((1,2,0))\n",
    "#         im2display = (im2display * 255).astype(np.uint8)\n",
    "#         plt.xticks([])\n",
    "#         plt.yticks([])\n",
    "#         plt.imshow(im2display)\n",
    "\n",
    "#     for j, item in enumerate(rotated):\n",
    "#         if j >= 9: break\n",
    "#         plt.subplot(5, 9, 9+j+1)\n",
    "#         im2display = item.transpose((1,2,0))\n",
    "#         im2display = (im2display * 255).astype(np.uint8)\n",
    "#         plt.xticks([])\n",
    "#         plt.yticks([])\n",
    "#         plt.imshow(im2display)\n",
    "        \n",
    "#     for j, item in enumerate(zoomed):\n",
    "#         if j >= 9: break\n",
    "#         plt.subplot(5, 9, 18+j+1)\n",
    "#         im2display = item.transpose((1,2,0))\n",
    "#         im2display = (im2display * 255).astype(np.uint8)\n",
    "#         plt.xticks([])\n",
    "#         plt.yticks([])\n",
    "#         plt.imshow(im2display)\n",
    "    \n",
    "    \n",
    "#     for j, item in enumerate(shifted):\n",
    "#         if j >= 9: break\n",
    "#         plt.subplot(5, 9, 27+j+1)\n",
    "#         im2display = item.transpose((1,2,0))\n",
    "#         im2display = (im2display * 255).astype(np.uint8)\n",
    "#         plt.xticks([])\n",
    "#         plt.yticks([])\n",
    "#         plt.imshow(im2display)\n",
    "    \n",
    "#     for j, item in enumerate(original):\n",
    "#         if j >= 9: break\n",
    "#         plt.subplot(5, 9, 36+j+1)\n",
    "#         im2display = item.transpose((1,2,0))\n",
    "#         im2display = (im2display * 255).astype(np.uint8)\n",
    "#         plt.xticks([])\n",
    "#         plt.yticks([])\n",
    "#         plt.imshow(im2display)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataloader):\n",
    "    phi = Phi(extractor_model)\n",
    "    features = phi.get_z_features(dataloader)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [03:35, 53.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# colored_features = extract_features(trainloader_color)\n",
    "# rotated_features = extract_features(trainloader_rotation)\n",
    "# zoom_features = extract_features(trainloader_zoom)\n",
    "# shift_features = extract_features(trainloader_shift)\n",
    "orig_features_full, orig_features_pca, feature_mapping, components = extract_features(trainloader_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian_features = extract_features(trainloader_gaussian)\n",
    "# uniform_features = extract_features(trainloader_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix_1_features = extract_features(trainloader_mix_1)\n",
    "# mix_2_features = extract_features(trainloader_mix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colored_features_2 = extract_features(trainloader_color_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset_orig.get_subset(\n",
    "    \"test\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize((448, 448)), transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "testloader = DataLoader(test_data, batch_size=batch_size,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(loader):\n",
    "    metadata_all = []\n",
    "    for i, (_, _, metadata) in tqdm(enumerate(loader)):\n",
    "        metadata_all.append(metadata)\n",
    "    metadata_all = np.vstack(metadata_all)\n",
    "    return metadata_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:25,  5.10s/it]\n"
     ]
    }
   ],
   "source": [
    "test_metadata = get_metadata(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [04:18, 51.66s/it]\n"
     ]
    }
   ],
   "source": [
    "test_features_full, tes_features_pca, _, _ = extract_features(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes_names = \"\"\n",
    "# for i, y in enumerate(labels):\n",
    "#     str_ = categories.loc[categories['y'] == i]['category'].tolist()[0]\n",
    "#     classes_names += str_\n",
    "#     if i < len(labels)-1:\n",
    "#         classes_names += \"_\"\n",
    "# classes_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"CLIP\"\n",
    "store_path = os.path.join(\"artifacts\", \"extracted_features\", dataset_name, f\"{suffix}\", f\"{len(labels)}_class\")\n",
    "if not os.path.isdir(store_path):\n",
    "    os.makedirs(store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def store_mapping(mapping):\n",
    "    with open(os.path.join(store_path,'pca_feature_mapping.pickle'), 'wb') as handle:\n",
    "        pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_mapping(feature_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_features(features, mode, type_):\n",
    "    np.save(os.path.join(store_path,f\"{type_}_{mode}_{features.shape[1]}.npy\"), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_features(colored_features, \"train\", \"color\")\n",
    "# store_features(rotated_features, \"train\",\"rotation\")\n",
    "# store_features(zoom_features, \"train\",\"zoom\")\n",
    "# store_features(shift_features, \"train\",\"shift\")\n",
    "store_features(orig_features_full, \"train\",\"orig_full\")\n",
    "store_features(orig_features_pca, \"train\",\"orig_pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_features(mix_1_features, \"train\", \"mix_1\")\n",
    "# store_features(mix_2_features, \"train\", \"mix_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_features(colored_features_2, \"train\", \"color_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_features(gaussian_features, \"train\", \"gaussian\")\n",
    "# store_features(uniform_features, \"train\", \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_features(test_features_full, \"test\", \"orig_full\")\n",
    "store_features(tes_features_pca, \"test\", \"orig_pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_features(test_metadata, \"test\", \"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_features(components, \"train\", \"pca_components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artifacts/extracted_features/waterbirds/CLIP/2_class'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
